---
title: "Основы статистики"
author: "Александр Нехаев"
documentclass: book
output:
    pdf_document:
        keep_tex: yes
        latex_engine: xelatex
        number_sections: true
        toc: yes
    html_document: default
mainfont: CMU Serif
fontsize: 10pt
---

```{r, include = FALSE}
# Area setup, libraries
library(tidyverse)
library(tidymodels)
library(modelr)
library(httr)
library(jsonlite)
library(gridExtra)
library(patchwork)
library(knitr)
library(distr)
library(gridExtra)
library(scatterplot3d)
library(GGally)

options(width = 60)
```

\newtheorem{theorem}{Теорема}[section]
\newtheorem{definition}{Определение}[section]

# Введение
## Понятие генеральной совокупности и выборки, репрезентативность выборки
### Понятие выборки и генеральной совокупности
\begin{definition}
Генеральная совокупность -- множество всех объеков относительно которой мы хотим делать выводы в рамках исследования некоторой проблемы.
\end{definition}

\begin{definition}
Выборка -- часть генеральной совокупности используемой в реальном исследовании.
\end{definition}

\begin{definition}
Репрезентативность выборки -- применимость выводов по выборке к генеральной совокупности.
\end{definition}

### Выборка
#### Простая случайная выборка (simple random sample)
Случайный выбор элементов из генеральной совокупности

#### Стратифицированная выборка (stratified sample)
Разбиение генеральной совокупности на несколько групп с явно различными свойствами. Затем случайной выборкой берем элементы из каждой группы.

#### Групповая выборка (cluster sample)
Так же разбиваем совокупность на кластеры, которые схожи по свойствам. Затем выбираем несколько кластеров, затем из кластеров выбираем случайные элементы.

## Типы переменных. Количественные и номинативные переменные

### Типы переменных
Все переменные характеризующие генеральные совокупности можно разделить на **количественные** и **номинативные**.

#### Количественные переменные
* Непрерывные
* Дискретные

Например: Непрерывная - рост человека из выборки на интервале от 160 до 190 см. Дискретная - количество детей в семье.

#### Номинативные
Используются для разделения элементов выборки на какие-то группы.

#### Ранговые переменные
Переменная к которой можно применять только операцию сравнения.

## Меры центральной тенденции
### Понятие описательной статистики
Гистограмма частот - первое описание формы распределения.
```{r, fig.height=3}
bind_rows(
    tibble(x = rnorm(200, mean = 8, sd = 1)),
    tibble(
        x = r(UnivarMixingDistribution(
            Norm(mean = 9, sd = 1),
            Norm(mean = 15, sd = 1)
        ))(200)
    ),
    .id = "id"
) |>
    ggplot(aes(x)) +
    facet_grid(cols = vars(id)) +
    geom_histogram(
        bins = 15,
        color = "white",
        fill = "orange"
    ) +
    labs(x = "", y = "")
```

#### Мера центральной тенденции
Отвечает на вопрос насколько высокие значения принимает переменная

### Мода
\begin{definition}
Мода – значение измеряемого признака, которая встречается максимально часто
\end{definition}
Пример: Пусть есть выборка:
```{r}
a <- c(
    185, 175, 170, 169, 171, 172, 175, 157, 170, 172, 167,
    173, 168, 167, 166, 167, 169, 172, 177, 178, 165, 161,
    179, 159, 164, 178, 172, 170, 173, 171
)
a_dot_plot <- tibble(a = a) |>
    ggplot(aes(a)) +
    geom_dotplot(binwidth = 1, fill = "orange")
a_dot_plot
```
```{r}
st.mode <- function(x) {
    u <- unique(x)
    tab <- tabulate(match(x, u))
    u[tab == max(tab)]
}
st.mode(a)
```

### Медиана
\begin{definition}
Медиана -- значение признака, которое делит упорядоченное множество данных пополам.
\end{definition}
```{r}
b <- c(157, 159, 161, 164, 165, 166, 167, 167, 167)
length(b)
```

```{r}
median(b)
```
В случае если у нас нечетное количество элементов -- все просто. Если нечетное, то берем среднее значение двух значений между которыми находится середина.
```{r}
median(a)
```

###  Среднее значение
\begin{definition}
Среднее значение -- сумма всех значений измеренного признака, деленая на количество измеренных значений.
\end{definition}
```{r}
mean(a)
```

### Выбор меры центральной тенденции
Смотрим на все значения, которые мы получили:
```{r}
tibble(x = a) |>
    summarise(
        Mode = st.mode(a),
        Median = median(a),
        Mean = mean(a)
    ) |>
    kable()
```

Если распределение симметрично, унимодально и не имеет заметных выбросов, то все тенденции дадут примерно одно значение.
Если оно симметрично, с выбросами или мультимодально, тогда лучше использовать моду или медиану.

### Свойства среднего
\begin{equation}
    M_{x+c}=M_x+c
\end{equation}
\begin{equation}
    M_{x+c}=M_x*c
\end{equation}
\begin{equation}
    \sum{(x_i - M_x)} = 0
\end{equation}
Проверка:
```{r}
cn <- round(rnorm(200))
c(
    mean(cn + 2) == mean(cn) + 2,
    mean(cn * 2) == mean(cn) * 2,
    round(sum(cn - mean(cn))) == 0
)
```

## Меры изменчивости
### Понятие меры изменчивости данных
Некоторые распределения имеют значительные отличия даже несмотря на близкие значения среднего, медианы и моды.
Для описания из различий используются меры изменчивости.

### Размах
\begin{definition}
Размах -- разность максимального и миниамального значений
\end{definition}
```{r}
a_dot_plot
max(a) - min(a)
```
Изменение краевых величин будет сильно влиять на эту меру.

### Дисперсия, стандартное отклонение
*Дисперсия -- средний квадрат отклонений индивидуальных значений признака от их средней величины.*

Среднее отклонение от среднего вообще говоря имеет вид:
$$
    \frac{\sum{(x_i - \overline{x})}}{n}
$$
Но как мы знаем из свойств среднего, числитель тогда будет равен 0. Исключаем отрицательные значения через возведение в квадрат:
\begin{equation}
    \frac{\sum{(x_i - \overline{x})^2}}{n}
\end{equation}
Это и называется дисперсией.
```{r}
var(a)
```
Однако мы взяли квадрат, что сильно влияет на результат (можно сказать, что поменялась размерность).
Поэтому более точной величиной будет корень из нее.

*Стандартное отклонение - корень из дисперсии.*
```{r}
sd(a)
```
Тут стоит отметить, что в литературе для обозначения стандартного отклонения для всей генеральной совокупности используется символ $\sigma$.
Для стандартного отклонения используется $sd$.

Еще момент. Если мы считаем дисперсию для всей генеральной совокупности, то мы смело используем формулу (0.1).
Однако если мы берем дисперсию для совокупности, то в знаменателе используем $n-1$.
Это связано со степенями свободы.

#### Пример
```{r}
example <- c(1, 2, 2, 3, 4, 4, 5)
mean(example)
(example - mean(example))^2
sum((example - mean(example))^2) / (length(example) - 1)
sqrt(sum((example - mean(example))^2) / (length(example) - 1))
```
Используя готовые функции:
```{r}
tibble(x = example) |>
    summarise(
        "Variance" = var(x),
        "Standard deviation" = sd(x)
    ) |>
    kable()
```
### Свойства дисперсии и стандартного отклонения
\begin{equation}
    D_{x+c} = D_{x}
\end{equation}
\begin{equation}
    sd_{x+c} = sd_{x}
\end{equation}
\begin{equation}
    D_{x*c} = D_x * c^2
\end{equation}
\begin{equation}
    sd_{x*c} = sd_x + c
\end{equation}

## Квартили распределения и график box-plot
### Квантили распределения
*Квантили - это такие значения признака, которые делят упорядоченные данные на некоторое число равных частей*
Примером квартиля является медиана, которую мы уже рассматривали, однако в статистике так же часто используются квартили распределения - 3 точки, которые делят распределение на 4 равных части.
### Квартили
*Квартили - три точки (значения признака), которые делят **упорядоченное** множество данных на 4 равные части.*
```{r}
sort(a)
quantile(a) |> kable()
```

### Box-plot
Верхняя и нижняя границы коробки отражают положение 1го и 3го квартилей.
Линия внутри коробки - 2й квартиль (медиана).
Положения усов - последние значения в пределе 1.5 межквартильных размахов от границ коробки.
Точки - выпадающие из них значения.
```{r, fig.width=2, fig.height=2.4}
tibble(a = a) |> ggplot(aes(y = a)) +
    geom_boxplot()
```

Теперь нанесем точки на график с box-plot:
```{r, fig.width=2, fig.height=4, fig.align='center'}
tibble(a = a) |>
    mutate(
        n = row_number(),
        x = (n / length(a)) * 0.4 - 0.25
    ) |>
    ggplot(aes(y = a)) +
    geom_boxplot() +
    geom_point(aes(x = x), color = "blue") +
    labs(x = "")
```
Box plot не столь информативен, сколько гистограмма, однако он помогает при сравнении двух распределений.

## Нормальное распределение
### Понятие нормального распределения
Характеристики:

* Унимодально
* Симметрично
* Отклонение наблюдений от среднего подчиняется определенному вероятностному закону:
    * В диапазоне от медианы до среднеквадратичного отклонения будет находиться примерно 34.1% всех значений.
    * В диапазоне от одного до двух среднеквадратичных отклонений будет находиться примерно 13.6%.
    * Вероятность встретить значение, превосходящее 3 стандартных отклонения весьма маловероятна (там около 0.1% значений)

### Стандартизация
*Стандартизация или z-преобразование – преобразование полученных данных в стандартную Z-шкалу (Z-scores)  со средним $M_z = 0$ и $D_z = 1$.*

Для этого:
$$
    Z_i = \frac{x_i-\overline{x}}{\sigma_x}
$$

Видим, что форма распределения не меняется, меняются только значения:
```{r}
tibble(a = a) |>
    ggplot(aes(x = a)) +
    geom_dotplot(binwidth = 1, fill = "orange")
tibble(a = base::scale(a)) |>
    ggplot(aes(x = a)) +
    geom_dotplot(
        binwidth = 1 / length(a),
        dotsize = 5,
        fill = "orange"
    )
```

### Правила двух и трех сигм, использование стандартизации
Ранее уже говорили, что:

* $M_x \pm \sigma \approx 68 \%$ наблюдений
* $M_x \pm 2\sigma \approx 95 \%$ наблюдений
* $M_x \pm 3\sigma \approx 100 \%$ наблюдений

z-преобразование позволяет ответить на вопрос какой процент наблюдений лежит в любом заданном диапазоне.

Пример: Мы хотим узнать какой процент значений превышает значение 154 если среднее значение составляет 150, а стандартное отклонение равно 8.

Находим z-значение для заданного значения:
```{r}
z <- (154 - 150) / 8
print(z)
pnorm(-z, mean = 0, sd = 1)
```

## Центральная предельная теорема
Допустим, что некоторый признак распределен нормально в генеральной совокупности имеет среднее значение и стандартное отклонение:
```{r}
mu <- 0
sigma <- 15
```

Обозначаем эту совокупность как
```{r}
dist <- rnorm(175 * 5000, mean = mu, sd = sigma)
```
и строим её:
```{r}
tibble(x = dist) |>
    ggplot(aes(x = x)) +
    geom_histogram(
        bins = 30,
        aes(y = after_stat(density)),
        color = "black", fill = "orange"
    ) +
    stat_function(
        fun = dnorm,
        args = list(mean = mu, sd = sigma),
        color = "#0088ff"
    )
```

Будем многократно извлекать из совокупности выборки по 175 значений каждая и замерять в них среднее значение и стандартное отклонение (отображены первые 9 выборок):
```{r}
sample_size <- 175
samples <- bind_rows(map(1:5000, \(id) tibble(x = dist) |>
    slice_sample(n = sample_size)), .id = "bin") |>
    mutate(bin = as.numeric(bin))
plt_names <- samples |>
    filter(bin <= 9) |>
    group_by(bin) |>
    summarise(mu = round(mean(x), 2), sd = round(sd(x), 2))

means <- plt_names$mu
names(means) <- plt_names$bin
sds <- plt_names$sd
names(sds) <- plt_names$bin

samples |>
    filter(bin <= 9) |>
    ggplot(aes(x = x)) +
    facet_wrap(vars(bin),
        scales = "free",
        labeller = label_bquote(
            "Sample " * .(bin) * " " * mu * " = " *
                .(means[bin]) * " " * sigma *
                " = " * .(sds[bin])
        )
    ) +
    geom_histogram(
        bins = 9,
        color = "black",
        fill = "orange"
    )
```
Теперь возьмем рассчитанные значения среднего и стандартного отклонения для всех выборок и отобразим их на графике:
```{r}
samples_data <- samples |>
    group_by(bin) |>
    summarise(
        mean = mean(x),
        sd = sd(x)
    )
samples_data |> ggplot(aes(x = mean)) +
    geom_histogram(
        aes(y = after_stat(density)),
        bins = 30, color = "black", fill = "orange"
    ) +
    geom_density(kernel = "gaussian", color = "#0088ff") +
    labs(
        title = bquote(paste(
            mu, "=", .(mean(samples_data$mean)),
            ", ", sigma, "=", .(sd(samples_data$mean))
        ))
    )
```
Значение $\sigma$ на этом графике называется **стандартной ошибкой среднего** и показывает на сколько в среднем значение выборочного среднего отклоняется от среднего генеральной совокупности.
С ростом количества элементов в выборке стандартная ошибка среднего будет уменьшаться.

Таким образом формулируем центральную предельную теорему.
\begin{theorem}
    Предположим исследуемый признак имеет нормальное распределение в генеральной совокупности с некоторым средним значением и стандартным отклонением и мы многократно извлекаем выборки равные по объему n  и в каждой выборке рассчитываем среднее значение после чего строим распределение средних значений.
    Такое распределение будет являться нормальным со средним, совпадающем со среднем генеральной совокупности и со стандартным отклонением, называемым стандартной ошибкой среднего и рассчитываемым по формуле:
    \begin{equation}
        se = \frac{\sigma}{\sqrt{n}}
    \end{equation}
\end{theorem}
**Замечание:** на самом деле исследуемый признак может иметь любое распределение и средние выборок так же буду распределены нормально.

Чем больше элементов в выборке, тем ближе среднее значение в каждой выборке к среднему значению генеральной совокупности и соответственно тем меньше будет стандартная ошибка среднего.
Так же есть правило, что если число элементов в выборке больше 30 и эта выборка репрезентативная, то формулу из теоремы
можно преобразовать до вида:
\begin{equation}
    se = \frac{sd(x)}{\sqrt{n}}.
\end{equation}

Пусть мы извлекли из совокупности всего одну выборку в 100 элементов. Стандартное отклонение 5 и среднее значение 3.
На основе этих данных мы можем предположить, как вели бы себя все выборки этой совокупности:
```{r}
5 / sqrt(100)
```
Соответственно распределение средних значений выборок имело бы вид:
```{r}
ggplot() +
    xlim(1, 5) +
    stat_function(
        fun = dnorm,
        args = list(mean = 3, sd = 5 / sqrt(100))
    )
```
Утверждается, что данное распределение будет получатся во всех выборках.

## Идея статистического вывода, p-уровень значимости
### Статистическая проверка гипотез
Как правило нас все таки интересуют гипотезы, а не конкретные значения.
Рассмотрим пример:

Предположим, что на выздоровление при некотором заболевании в среднем требуется $M = 20$ дней, но мы разработали новый препарат и хотим проверить может ли он сократить этот срок.
Мы взяли 64 пациента и опробовали на них новый метод лечения.
Оказалось, что средняя скорость выздоровления сократилась до $x = 18.5$ дней при среднем стандартном отклонении $sd = 4$.
Какой вывод можно сделать из этих данных?

С одной стороны судя по значениям, мы действительно сократили срок выздоровления.
С другой стороны, такой результат мог быть получен случайно и без препарата.
Введем несколько важных понятий.

В этом исследовании будут конкурировать 2 гипотезы:

* $H_0$ -- никакого реального воздействия препарат не оказывает и на самом деле среднее значение генеральной совокупности тех пациентов, которые получили препарат на самом деле не отличается от генеральной совокупности всех больных, $M_{\text{НП}} =20$.
* $H_1$ -- препарат влияет и среднее значение скорости восстановления генеральной совокупности всех пациентов, использующих препарат отличается, $M_{\text{НП}} \neq 20$.

Пусть на самом деле верна первая гипотеза.
Тогда согласно ЦПТ если бы мы многократно повторяли исследование, то выборочные средние распределились бы нормальным образом вокруг среднего генеральной совокупности с стандартной ошибкой
$se = \frac{sd}{\sqrt{n}} = \frac{4}{\sqrt{64}} = 0.5$.
Теперь отвтим на вопрос <<насколько далеко наше выборочное среднее отклонилось от предполагаемого среднего значения генеральной совокупности в единицах стандартного отклонения?>>
Для этого сделаем z-преобразование:
$$
    z = \frac{\overline{x}  - M}{se} = \frac{18.5 - 20}{0.5} = -3
$$
Это означает, что если бы в генеральной совокупности среднее значение на самом деле равнялось бы 20, то выборочно среднее отклонилось бы от среднего генеральной совокупности на $-3\sigma$ влево.

### Идея статистического вывода
Теперь воспользуемся свойствами нормального распределения чтобы рассчитать вероятность такого или еще более сильно выраженного отклонения от среднего значения:
```{r}
pnorm(-3) + pnorm(3, lower.tail = FALSE)
ggplot() +
    xlim(-4, 4) +
    stat_function(fun = dnorm) +
    geom_area(
        stat = "function", fun = dnorm,
        fill = "royalblue", xlim = c(-4, -3), aes(c(-4, 4))
    ) +
    geom_area(
        stat = "function", fun = dnorm,
        fill = "royalblue", xlim = c(3, 4), aes(c(-4, 4))
    )
```

Итак, на первом этапе мы предположили, что на самом деле верна нулевая гипотеза.
Если это так, то все выборочные средние распределились бы вокруг среднего генеральной совокупности, которое, как мы предполагаем, равняется 20.
Однако в нашем эксперименте выборочное среднее оказалось равно 18.5.
Зная стандартную ошибку среднего мы смогли рассчитать вероятность получить такое или еще более сильно выраженное отклонение причем как в правую, так и в левую сторону.
Оказалось, что такая вероятность $\approx 0.003$.

Таким образом основная **идея статистического вывода** заключается в следующем: мы допускам, что верна нулевая гипотеза и на самом деле никаких различий у нас нет.
Затем мы считаем вероятность того, что мы получили такие или еще более сильно выраженные различия абсолютно случайно.
Это значение в статистике называется *p*-уровень значимости и именно при помощи этого показателя мы выясним какую гипотезу считать более состоятельной.
Чем меньше *p*-уровень, тем больше оснований отклонить
нулевую гипотезу.
Считается, что если $p <0.05$, то можно смело принимать альтернативную гипотезу.
Однако если *p*-уровень больше этого порога, у нас недостаточно оснований отклонить эту гипотезу.

### *p*-уровень значимости и его интерпретация
Получаем, что у нас достаточно оснований для отклонения нулевой гипотезы.
Вопрос - зачем рассчитывать значение отклонения в принципиально другом направлении?
Ведь если мы проверили гипотезу о том, что препарат ускорит скорость выздоровления, то зачем учитывать вероятность, что он её понизит? 
Вероятность это площадь под кривой.
Если рассматривать только одно направление, то вероятность события будет меньше.
Тем не менее принято учитывать оба конца распределения.
Реально мы не знаем в какую сторону мы получим отклонение от среднего и от такого развития событий никто не застрахован.
Иногда действительно используется односторонний *p-*критерий, обычно если отклонение в другую сторону невозможно.

В реальности *p-*уровень означает, что если верна нулевая гипотеза, то вероятность получить такие или еще более выраженные
различия будет равна *p-*уровню.
Он ничего не говорит о силе эффекта.
Мы можем получить в среднем сокращение времени болезни на неделю, но при этом не значимое с точки зрения статистики.

Что делать, если уровень значимости оказался больше 0.05? Вывод простой - у нас недостаточно оснований для отклонения
нулевой гипотезы.
Сам по себе *p-*уровень ничего не говорит ни о правильности, ни о ценности получаемых результатов.

Основная идея статистической проверки гипотезы подразумевает, что мы иногда будем совершать статистические ошибки 1го и 2го рода.

\begin{definition}
    \textbf{Ошибка 1-го рода} подразумевает, что мы отклонили первую гипотезу, хотя на самом деле она верна.
\end{definition}

\begin{definition}
    \textbf{Ошибка 2-го рода} подразумевает, что мы не отклонили нулевую гипотезу, хотя на самом деле она не верна.
\end{definition}

# Сравнение средних
## T-распределение
### Нормальное распределение и ограниченность количества наблюдений
С выборками в которых большое число элементов все понятно.
Теперь рассмотрим ситуацию, когда число элементов достаточно мало (<30).
Особенность такого случая заключается в том, что нарушается предположение о том, что во-первых, стандартное отклонение среднего уже не такое хорошее, а во-вторых нарушается предположение о том, что все выборочные средние будут вести себя в соответствии с нормальным распределением.

### Распределение Стьюдента (t-распределение)
По этим причинам, если число наблюдений невелико и стандартное отклонение неизвестно, то используется распределение Стьюдента (t-распределение).

Распределение Стьюдента унимодально и симметрично, но наблюдения с большой вероятностью попадают за пределы $\pm 2 \sigma$ от $M$.
```{r}
ggplot() +
    xlim(-4, 4) +
    stat_function(fun = dnorm, aes(color = "Normal")) +
    stat_function(
        fun = dt, args = list(df = 1),
        aes(color = "Student")
    ) +
    labs(
        x = "x",
        y = "density",
        color = "Legend"
    ) +
    scale_color_manual(values = c(
        "Normal" = "blue",
        "Student" = "orange"
    ))
```

Форма распределения определяется числом степеней свободы $(df = n -1)$, где $n$ - число наблюдений в выборке.
С увеличением числа $df$ распределение стремится к нормальном.
```{r}
t_student_demo_plot <- ggplot() +
    xlim(-4, 4) +
    stat_function(
        fun = dt, args = list(df = 1),
        aes(color = "df = 1")
    ) +
    stat_function(
        fun = dt, args = list(df = 3),
        aes(color = "df = 3")
    ) +
    stat_function(
        fun = dt, args = list(df = 8),
        aes(color = "df = 8")
    ) +
    stat_function(
        fun = dt, args = list(df = 30),
        aes(color = "df = 30")
    ) +
    stat_function(fun = dnorm, aes(color = "Normal")) +
    labs(x = "x", y = "density", color = "Legend") +
    scale_color_manual(values = c(
        "df = 1" = "royalblue",
        "df = 3" = "orange",
        "df = 8" = "green",
        "df = 30" = "red",
        "Normal" = "purple"
    ))
t_student_demo_plot
```

Рассмотрим пример.
Пусть в генеральной совокупности среднее значение $\mu=10$.
На выборке получили среднее равное $x = 10.8$ со стандартным отклонением $sd = 2$ при числе испытаний $N =25$.

Если пользоваться стандартной формулой описанной ранее, то мы бы сказали, что в соответствии с ЦПТ все выборочные средние распределились бы нормально вокруг среднего генеральной совокупности и стандартная ошибка среднего была бы: 

```{r}
2 / sqrt(25)
```
Теперь мы хотим посмотреть насколько наше выборочное среднее отклонилось от среднего генеральной совокупности.
Тогда мы сможем найти вероятность получить такое или еще более выраженное отклонение.
Для этого ищем соответствующее z-значение:
```{r}
(10.8 - 10) / 0.4
```
То есть отклонение составляет 2 стандартных отклонения.

Теперь чуть больше поговорим о том, почему t-распределение необходимо на небольшом объеме выборки.

#### Про необходимость t-критерия
Мы знаем, что если некоторый признак в генеральной совокупности распределен нормально (или согласно какому-либо другому распределению) со средним $\mu$ и стандартным отклонением $\sigma$ и мы будем многократно извлекать выборки одинакового размера $n$, и для каждой выборки рассчитывать, как далеко выборочное среднее $\overline{X}$ отклонилось от среднего в генеральной совокупности в единицах стандартной ошибки среднего:
$$
    z = \frac{\overline{X} - \mu}{\frac{\sigma}{\sqrt(n)}},
$$
то эта величина $z$ будет иметь стандартное нормальное распределение со средним равным нулю и стандартным отклонением равным единице.

Обратим внимание,  что для расчета стандартной ошибки мы используем именно стандартное отклонение в генерально совокупности - $\sigma$.
Ранее мы уже обсуждали, что на практике $\sigma$ нам практически никогда не известна, и для расчетов стандартной ошибки мы используем выборочное стандартное отклонение.

Строго говоря в таком случае распределение отклонения выборочного среднего и среднего в генеральной совокупности, деленного на стандартную ошибку, теперь будет описываться именно при помощи t-распределения.
$$
    t = \frac{\overline{X} - \mu}{\frac{sd}{\sqrt(n)}},
$$

Таким образом, в случае неизвестной $\sigma$ мы всегда будем иметь дело с t-распределением.
На этом этапе возникает вопрос, почему в предыдущей главе использовался $z$-критерий для проверки гипотез, используя выборочное стандартное отклонение?

Мы уже знаем, что при довольно большом объеме выборки $(n >30)$ t-распределение совсем близко подбирается к нормальному распределению:
```{r}
t_student_demo_plot
```
Поэтому иногда для простоты расчетов говорится, что если $n >30$, то мы будем использовать свойства нормального распределения для наших целей.
Строго говоря, это, конечно, неправильный подход, который часто критикуют.
В до компьютерную эпоху этому было некоторое объяснение, чтобы не рассчитывать для каждого $n$ большего 30 соответствующее критическое значение t-распределения, статистики как бы округляли результат и использовали нормальное распределение для этих целей.
Сегодня с этим больше проблем нет и все статистические программы без труда рассчитывают все необходимые показатели для t-распределения с любым числом степеней свободы.
Действительно при выборках очень большого объема t-распределение практически не будет отличаться от нормального, однако хоть и очень малые различия все равно будут.

Поэтому, правильнее будет сказать, что мы используем t-распределение не потому что у нас маленькие выборки, а потому что мы не знаем стандартное отклонение в генеральной совокупности.
Поэтому в дальнейшем мы всегда будем использовать t-распределение для проверки гипотез, если нам неизвестно стандартное отклонение в генеральной совокупности, необходимое для расчета стандартной ошибки, даже если объем выборки больше 30.

Если мы допустили, что все выборочные средние будут распределены нормальным образом, то вероятность получить отклонение превышающее $2\sigma$ как в левую, так и в правую сторону будет составлять:
```{r}
pnorm(-2) + pnorm(2, lower.tail = FALSE)
ggplot() +
    xlim(-4, 4) +
    stat_function(fun = dnorm) +
    geom_area(
        stat = "function", fun = dnorm,
        fill = "royalblue", xlim = c(-4, -2), aes(c(-4, 4))
    ) +
    geom_area(
        stat = "function", fun = dnorm,
        fill = "royalblue", xlim = c(2, 4), aes(c(-4, 4))
    )
```
То есть *p*-уровень значимости будет меньше чем 0.05 и мы смело сможем отклонить нулевую гипотезу, согласно которой наша выборка принадлежит генеральной совокупности со среднем значением 10.8.
Но как мы сказали при небольшом объеме выборки распределение выборочного среднего будет отличаться от нормального и вероятность получить более выраженное отклонение от среднего станет выше.
Рассчитаем данную вероятность предположив, что мы работаем с t-распределением с 24 степенями свободы.
```{r}
pt(-2, df = 24) + pt(2, df = 24, lower.tail = FALSE)
ggplot() +
    xlim(-4, 4) +
    stat_function(fun = dt, args = list(df = 24)) +
    geom_area(
        stat = "function", fun = dt, args = list(df = 24),
        fill = "royalblue", xlim = c(-4, -2), aes(c(-4, 4))
    ) +
    geom_area(
        stat = "function", fun = dt, args = list(df = 24),
        fill = "royalblue", xlim = c(2, 4), aes(c(-4, 4))
    )
```
Это означает, что если бы мы пользовались t-распределением, то нулевую гипотезу мы бы отклонить не смогли.
t-критерий рассчитывается так же как z-критерий:
$$
    t = \frac{\overline{x} - M}{\frac{sd}{\sqrt(n)}},
$$

Однако если бы мы в этом случае получили 2, то в t-распределении с 24 степенями свободы $p =0.056$ и нулевую гипотезу мы бы отклонить не смогли.

### Понятие числа степеней свободы
Как уже понятно t-распределение зависит от числа наблюдений в выборке как $n - 1$.
Если дать более общее определение, то число степеней свободы - это число элементов которые могут варьироваться при расчете некоторого статистического показателя.

## Сравнение двух средних, t-критерий Стьюдента
### Сравнение двух средних
Критерий который позволяет сравнивать между собой два выборочных средних называется парным t-тестом или просто t-критерием Стьюдента.

### t-критерий Стьюдента
\begin{definition}
Критерий который позволяет сравнивать между собой два выборочных средних называется \textbf{парным t-тестом} или просто \textbf{t-критерием Стьюдента}.
\end{definition}

### t-критерий Стьюдента
Предположим мы хотим сравнить два средних выборочных значения $\overline{x_1}$ рассчитанное на выборке со стандартным отклонением $sd_1$ и числом элементов выборки $n_1$ и $\overline{x_2}$ с $sd_2$ и $n_2$.
Сначала сформулируем статистические гипотезы:

* $H_0$ - в генеральной совокупности никакого различия между этими значениями нет и $\mu_1 = \mu_2$.
* $H_0$ - $\mu_1 \ne \mu_2$.

Предположим, что верна нулевая гипотеза.
Если это так, то при многократном повторении эксперимента и каждый раз рассчитывали разность между двумя выборочными средними значениями $\overline{x_1} - \overline{x_2}$, то эта величина распределилась бы следующим образом: мы бы получили симметричное распределение с средним значением $\mu_1 - \mu_2 = 0$ и стандартной ошибкой $se = \sqrt{\frac{sd_1^2}{n_1}+\frac{sd_2^2}{n_2}}$.
Причем это распределение будет t-распределением с числом степеней свобод, вычисляемым по формуле $df = n_1 + n_2 - 2$.
На основе этой информации мы можем рассчитать насколько далеко конкретно наша разность между двумя средними значениями отклонилась от предполагаемого показателя генеральной совокупности и тем самым рассчитать вероятность получить такие или еще более сильные отклонения при условии, что на самом деле нулевая гипотеза верна.

Окончательная формула для t-критерия будет иметь вид:
\begin{equation}
t = \frac{(\overline{x_1}-\overline{x_2})-(\mu _1-\mu
   _2)}{\sqrt{\frac{\text{sd}_1^2}{n_1}+\frac{\text{sd}_2^2}{n_2}}}
\end{equation}
На основе этого показателя и зная число степеней свобод $(df =n_1 +n_2 -2)$ мы можем рассчитать p-уровень значимости, который покажет нам вероятность получить такое или еще более выраженное отклонение при условии, что нулевая гипотеза верна.

**Пример:** Процесс денатурации ДНК представляет разрушение водородных связей между двумя цепями этой молекулы и очень сильно зависит от температуры, с которой мы воздействуем на молекулу.
```{r, echo = FALSE}
ds <- tibble(
    "1" = c(
        84.7, 105.0, 98.9, 97.9, 108.7, 81.3, 99.4, 89.4, 93.0,
        119.3, 99.2, 99.4, 97.1, 112.4, 99.8, 94.7, 114.0, 95.1, 115.5, 111.5
    ),
    "2" = c(
        57.2, 68.6, 104.4, 95.1, 89.9, 70.8, 83.5, 60.1,
        75.7, 102.0, 69.0, 79.6, 68.9, 98.6, 76.0, 74.8, 56.0, 55.6, 69.4, 59.5
    )
)
kable(ds, caption = "Температуры плавления ДНК (датасет ds)")
```
При сравнении двух видов между собой были получены следующие различия в средней температуре плавления ДНК:
```{r}
ds_f <- ds |> pivot_longer(everything(),
    names_to = "dna", values_to = "val"
)
ds_stats <- ds_f |>
    group_by(dna) |>
    summarise(m = mean(val), sd = sd(val), n = length(val))
kable(ds_stats)
```
Формулируем гипотезы:

* $H_0$: $\mu_1 = \mu_2$
* $H_1$: $\mu_1 \ne \mu_2$

Считаем t-критерий:
```{r}
t <- ds_stats |>
    mutate(dv = sd^2 / n) |>
    summarise(
        m.m = reduce(m, ~ .x - .y),
        m.dv = sqrt(sum(dv))
    ) |>
    mutate(t = m.m / m.dv) |>
    pull()
```
Число степеней свобод:
```{r}
df_val <- sum(ds_stats$n) - 2
df_val
```
Считаем интересующую нас вероятность:
```{r}
p <- pt(-t, df = df_val) + pt(t, df = df_val, lower.tail = FALSE)
p
```
Через встроенный функционал можно сильно проще:
```{r}
t.test(val ~ dna, data = ds_f, var.equal = TRUE)$p.value
```
Это меньше чем пороговое значение для p.
Таким образом мы обнаружили статистически значимое различие в средней температуре плавления ДНК двух видов.

### Построение графиков
Для начала как делать не надо (что это вообще такое?):
```{r}
ggplot(ds_stats, aes(x = dna, y = m, fill = dna)) +
    geom_col()
```
Как лучше:
```{r}
ggplot(ds_f, aes(dna, val)) +
    geom_boxplot() +
    labs(
        title = "DNA melting temperature",
        x = "Type", y = "Temperature, F"
    )
```

## Проверка распределений на нормальность, QQ-Plot
### Сравнение распределения с нормальным
Требование к нормальному распределению очень часто встречается в статистике при использовании различных методов.
Как оценить отличие распределения от нормального?

Один из самых простых способов - построить гистограмму частот признака и поверх наложить кривую идеального нормального распределения.
Например:
```{r}
dist <- rnorm(1000)
ggplot(tibble(a = dist), aes(dist)) +
    geom_histogram(
        bins = 32, aes(y = after_stat(density)),
        fill = "orange", color = "black"
    ) +
    stat_function(fun = dnorm)
```

### QQ-plot
Еще один графический способ проверить распределение на нормальность -- QQ-plot.
```{r}
ggplot(tibble(a = dist), aes(sample = dist)) +
    geom_qq() +
    geom_qq_line() +
    labs(x = "Theoretical Quantiles", y = "Sample Quantiles")
```

QQ-plot  удобно использовать, когда число наблюдений невелико.
Данных для построения гистограммы мало и удобно анализировать каждое значение отдельно что и возможно по такому графику.

Попробуем проверить этот метод на задаче с температурой плавление ДНК у видов.
```{r}
hist_verify_plot <- function(x) {
    m <- mean(x$val)
    std <- sd(x$val)
    dna_n <- unique(x$dna)
    return(x |> ggplot(aes(val)) +
        xlim((m - 3 * std), (m + 3 * std)) +
        geom_histogram(
            bins = 8, aes(y = after_stat(density)),
            color = "black", fill = "orange"
        ) +
        stat_function(fun = dnorm, args = list(
            mean = mean(x$val),
            sd = sd(x$val)
        )) +
        labs(title = str_c("DNA ", dna_n)))
}

verify_qq <- function(x) {
    return(
        x |> ggplot(aes(sample = val)) +
            geom_qq() +
            geom_qq_line()
    )
}

suppressWarnings(grid.arrange(
    hist_verify_plot(filter(ds_f, dna == 1)),
    verify_qq(filter(ds_f, dna == 1)),
    hist_verify_plot(filter(ds_f, dna == 2)),
    verify_qq(filter(ds_f, dna == 2)),
    nrow = 2, ncol = 2
))
```

### Тест Шапиро-Уилка
Тест Шапиро-Уилка позволяет определить, что выборка изъята из генеральной совокупности и её распределение соответствует нормальному.
```{r}
ds_f |>
    group_by(dna) |>
    summarise(
        swt = shapiro.test(val)$p.value
    ) |>
    kable()
```

Если уровень SWT > 0.05 это хорошо и не позволяет отклонить нулевую гипотезу.

### Проблема выбросов
Посмотрим как влияет появление выбросов на параметры выборки.
Для этого добавим по одному выбросу в каждый из наборов значений в задаче про температуру плавления и сравним результаты со старыми.
```{r}
ds_damaged <- ds_f |> bind_rows(
    tibble(dna = c("1", "2"), val = c(60, 130))
)

ggplot(ds_damaged, aes(dna, val)) +
    geom_boxplot() +
    labs(
        title = "DNA melting temperature",
        x = "Type", y = "Temperature, F"
    )

suppressWarnings(grid.arrange(
    verify_qq(filter(ds_damaged, dna == 1)),
    verify_qq(filter(ds_damaged, dna == 2)),
    nrow = 1, ncol = 2
))

ds_damaged |>
    group_by(dna) |>
    summarise(
        m = mean(val),
        sd = sd(val),
        n = length(val),
        swt = shapiro.test(val)$p.value
    ) |>
    kable()
```

### U-критерий Манна-Уитни
Что делать если распределение признака сильно отличается от нормального и мы опасаемся применять t-признак Стьюдента?
В таких ситуациях используется непараметрический аналог, называемый U-критерием Манна-Уитни.
```{r}
ds_damaged |>
    mutate(
        dna = str_c("DNA", dna)
    ) |>
    group_by(dna) |>
    mutate(row = row_number()) |>
    pivot_wider(
        names_from = dna,
        values_from = val
    ) |>
    summarise(
        MW = wilcox.test(DNA1, DNA2, exact = FALSE)$p.value
    ) |>
    pull()
```

## Однофакторный дисперсионный анализ
### Расчет на практическом примере
Предположим, что у нас есть следующий набор данных:
```{r}
ds <- tibble(
    "1" = c(3, 1, 2),
    "2" = c(5, 3, 4),
    "3" = c(7, 6, 5)
)
ds_f <- ds |>
    pivot_longer(
        cols = everything(),
        names_to = "name",
        values_to = "vals"
    )
kable(ds)
```
Сформулируем гипотезы:

* $H_0: \mu_1 = \mu_2 = \mu_3$
* $H_1: \mu_1 \neq \mu_2 \neq \mu_3$

Среднее по всему набору:
```{r}
ds_f |>
    summarise(
        mean = mean(vals)
    ) |>
    pull()
```
Введем понятие SST -- общая сумма квадратов.
Этот показатель позволяет увидеть насколько высока изменчивость данных.
```{r}
ds_f |>
    summarise(
        sst = sum((vals - mean(vals))^2)
    ) |>
    pull()
```
Число степеней свободы для всего набора данных будет составлять $8 (n -1)$.
Так же есть два важных показателя: SSW и SSB.
SSW -- это сумма квадратов внутри выборки:
```{r}
ssw <- ds_f |>
    group_by(name) |>
    summarise(
        ssw = sum((vals - mean(vals))^2)
    ) |>
    ungroup() |>
    summarise(
        ssw = sum(ssw)
    ) |>
    pull()
```
Число степеней свободы для SSW - это сумма степеней свободы для каждой группы.
В данном случае соответственно - 6.
Еще один показатель: SSB - сумма квадратов меж выборок.
Вычисляется как:
```{r}
ssb <- ds_f |>
    group_by(name) |>
    summarise(
        n = length(vals),
        mean = mean(vals)
    ) |>
    mutate(
        means_all = mean(ds_f$vals),
        pre_ssb = n * (mean - means_all)^2
    ) |>
    summarise(
        ssb = sum(pre_ssb)
    ) |>
    pull()
```

### F-значение
Введем так же $F$-value -- показатель Фишера.
Он вычисляется по формуле:

\begin{equation}
    F = \frac{SSB/df_{SSB}}{SSW/df_{SSW}}
\end{equation}

В нашем случае будет иметь значение:
```{r}
f <- (ssb / 2) / (ssw / 6)
f
```

Отметим так же, что отношение $\frac{SSB}{ds_{SSB}}\rightarrow 0$ с ростом числа элементов, из чего следует, что $F \rightarrow 0$ с ростом числа элементов.
Наконец, основываясь на значении показателя Фишера, мы так же можем оценить отклонение значений от нулевой гипотезы:
```{r}
pf(f, df1 = 2, df2 = 6, lower.tail = FALSE)
```
Или через готовую функцию в языке R:
```{r}
one_way <- aov(vals ~ name, data = ds_f) |> tidy()
one_way |> kable()
drop_na(one_way)$p.value
```
В целом ANOVA позволяет сравнивать значимости различий для произвольного количества групп.
Характерный вид распределения Фишера представлен ниже.
Так же отметим, что он считается в одном направлении, поскольку само распределение Фишера имеет только положительные значения.
```{r}
ggplot() +
    xlim(0, 3) +
    stat_function(
        fun = stats::df,
        args = list(df1 = 5, df2 = 10)
    )
```

### Применение и интерпретация
Генотерапия позволяет корректировать работу дефектного гена, ответственного за развитие заболевания.
В эксперименте сравнивалась эффективность четырех различных типов терапии.
Результаты исследования представлены в таблице:
```{r}
genetherapy <- read_csv("genetherapy.csv", show_col_types = FALSE)
gene_descr <- genetherapy |>
    group_by(Therapy) |>
    summarise(
        N = length(expr),
        M = mean(expr),
        SD = sd(expr)
    )
kable(gene_descr)
```
Вводим гипотезу: $H_0: \mu_1 = \mu_2 = \mu_3 = \mu_4$.
Применим ANOVA:
```{r}
gene_anova <- aov(expr ~ Therapy, data = genetherapy) |> tidy()
kable(gene_anova)
```
Здесь sumsq то же самое, что и SSB.
meansq -- $\frac{SSB}{df}$.
Как видим по результатам p.value -- нулевая гипотеза отклоняется (хотя бы 2 средних отличаются между собой).
Теперь посмотрим график:
```{r}
genetherapy |>
    ggplot(aes(Therapy, expr)) +
    geom_boxplot()
```

## Множественные сравнения в ANOVA
### Проблема множественного сравнения выборок
Проблема множественного сравнения возникает когда нужно сравнить множество выборок между собой.
Почему при этом нельзя попарно использовать критерий Стьюдента?
Для этого приводим пример:
Пусть есть генеральная совокупность со средним 0 и стандартным отклонением 1.
Из этой совокупности мы будем многократно извлекать выборки и сравнивать их между собой.
Функция для изъятия выборок имеет следующий вид:
```{r}
false_alarm <- function(m, n, a) {
    tt_test <- function(x, y) {
        return(t.test(x, y, var.equal = TRUE)$p.value)
    }
    d <- matrix(0, n, m)
    s <- combn(m, 2)
    x <- vector("logical", 1000)
    for (i in 1:1000) {
        d <- apply(d, 2, function(x) rnorm(n))
        for (j in 1:ncol(s)) {
            test <- tt_test(d[, s[1, j]], d[, s[2, j]])
            if (test <= a) {
                x[i] <- TRUE
                break
            }
        }
    }
    tx <- tibble(x = x) |>
        group_by(x) |>
        count()
    tx |> ggplot(aes(x = x, y = n, fill = x)) +
        geom_col() +
        labs(
            title = tx |>
                filter(x == TRUE) |>
                select(c(n)) |>
                pull() / 1000
        )
}

false_alarm(2, 30, 0.05)
```
На графике показано в каком проценте случаев мы получили статистически значимые различия.
В 5% случаев мы получили статистически значимые различия из одной выборки.
То есть на самом деле никаких статистически значимых различий мы не должны были получить.
Но поскольку мы выбрали некоторый порог p-уровня значимости после которого мы принимаем различия достоверными.
Теперь увеличим количество выборок:
```{r}
false_alarm(8, 30, 0.05)
```
Теперь уже в 52% случаев мы получим хотя бы одно статистически значимое различие между 8 выборками несмотря на то, что они из одной генеральной совокупности.
Теперь посмотрим что будет если бы сделаем сравнение в 20 выборках:
```{r}
false_alarm(20, 30, 0.05)
```
Когда мы сравниваем между собой 2 группы мы принимаем различия значимыми если показатель p меньше 0.05.
То есть даже если различий на самом деле нет, в 5% случаев мы бы получили различия случайно.
Но когда мы сравниваем 4 комбинации оказывается, что вероятность получить одно различие уже значительно больше.
Если мы увеличим количество групп, то вероятность получить различия будет стремиться к единице.
Это означает, что если мы многократно увеличиваем количество сравниваемых групп, то вероятность получить хотя бы одно различие, которого на самом деле вообще не существует очень сильнол увеличивается.
Таким образом нам нужно корректировать выбор p-уровня.
Такой поправкой является поправка Бонферрони.
```{r}
false_alarm(20, 30, 0.05 / 28)
```
Мы вернулись к тому же уровню, что и при 2 сравнениях.
Аналогичная ситуация будет и при 20 группах:
```{r}
comb_count <- ncol(combn(20, 2))
false_alarm(20, 30, 0.05 / comb_count)
```
При достаточном уровне терпения можно всегда получить значимые различия. 
Допустим мы не смогли отвергнуть нулевую гипотезу и стали добавлять большое число других факторов (пол, место рождения, возраст, семейное положение и тд) и продолжали сравнивать испытуемых по уровню экспрессии гена и на определенном этапе получим значимую связь у испытуемых по месту рождения и сделать выводы о влиянии внешней среды.
Проблема в том, что при поправке Бонферрони при 100 сравнениях гарантируется отсутстиве хотя бы одного ложного результата, но при этом упускается около 80% реальных открытий.
Поэтому она сильно критикуется: она так понижает уровень значимости, что получить различия зачастую становится невозможным.
Но современная статистика вынуждена работать с большим количеством сравнений. Поэтому разработана серия методов, которые работают лучше и возволяет не так сильно понижать p-уровень.

### Критерий Тьюки
Возвращаемся к примеру со сравнением 4х типов терапии.
Критерий Тьюки очень похож на TTest, однако инчае рассчитывается
стандартная ошибка.
С помощью некго можно рассчитать доверительный интервал между средними значениями
$$
\overline{x_A} - \overline{x_B}
$$
и если такой доверительный интервал не включает в себя 0, то можно отклонить нулевую гипотезу о равенстве средних.
```{r}
gene_anova_tukey <- aov(expr ~ Therapy, data = genetherapy) |>
    TukeyHSD() |>
    tidy() |>
    select(-c(null.value))
kable(gene_anova_tukey, digits = 4)
```
Видим, что значимо отличаются группы C-A, D-A, D-B:
```{r}
gene_anova_tukey |>
    filter(adj.p.value < 0.05) |>
    kable(digits = 4)
```

### Интерпретация результатов
Таким образом, если мы сравнили множество групп и не применили поправку, то получаем критику в свой адрес.
Если мы применим поправку Бонферрони и остались те же значимые различия, то никаких претензий быть не может.
Так же есть более свободные поправки.
Более важный вопрос - это зачем вообще производится сравнение.
Если мы проверяем какую-то гипотезу и не нашли подтверждений, то можно додавить расчеты так, чтобы что-то там все таки отыскать.
Надо так же смотреть на дополнительные факторы.
Зачастую значимые различия возникают за счет множественного сравнения, так что нужно применять поправку.

## Многофакторный ANOVA
### Двухфакторный дисперсионный анализ
**Задача:**
Атеросклероз довольно опасное заболевание - причина ишемичной болезни сердца и инсультов.
Анализ экспрессии генов лейкоцитов позволяет предсказать вероятность развития данного заболевания.
В эксперименте исследовался уровень экспрессии в зависимости от возраста пациентов и дозировки лекарства аторвастатина.

```{r}
data_ath <- read_csv("atherosclerosis.csv",
    show_col_types = FALSE
) |>
    mutate(
        age = factor(case_when(
            age == 1 ~ "Young",
            age == 2 ~ "Old",
        )),
        dose = factor(case_when(
            dose == "D1" ~ "High",
            dose == "D2" ~ "Low",
        ))
    )
ath_stat <- data_ath |>
    group_by(age, dose) |>
    summarise(
        N = dplyr::n(),
        Mx = mean(expr),
        SD = sd(expr)
    )
kable(ath_stat)
```
Результаты дисперсионного анализа:
```{r}
ath_anova <- aov(expr ~ age + dose, data = data_ath) |> tidy()
kable(ath_anova)
```
Теперь строим график и интерпретируем результат:
```{r}
data_ath |>
    ggplot(aes(dose, expr, fill = age)) +
    geom_boxplot()
```
Значимым является только фактор возраста (обращаем внимание, что в ANOVA p-уровень ниже 0.05 только у группы по возрасту) потому что вне зависмости от дозировки, молодые участники оказались с более высоким фактором переменной чем пожилые.
Таким образом препарат значимый для фактора возраста, но не значимый для фактора дозировки.
Возможна ситуация, когда значимы оба фактора.

### Взаимодействие факторов в ANOVA
Чтобы познакомиться с взаимодействием факторов рассмотрим еще один пример:
Исследователей интересовало влияние инъекции некоторого гормона на показатель концентрации кальция в плазме крови у птиц с учетом их пола.
В таблице представлены данные экспериментальной и контрольной группы.
```{r}
data_birds <- read_csv("birds.csv",
    show_col_types = FALSE
) |>
    mutate(
        hormone = factor(case_when(
            hormone == 0 ~ "Hormone 1",
            hormone == 1 ~ "Hormone 2",
        )),
        sex = factor(case_when(
            sex == 0 ~ "Male",
            sex == 1 ~ "Female"
        ))
    )
birds_stat <- data_birds |>
    group_by(hormone, sex) |>
    summarise(
        N = dplyr::n(),
        Mx = mean(var4),
        SD = sd(var4)
    )
birds_stat |> kable()
```
Результаты дисперсионного анализа:
```{r}
birds_anova <- aov(
    var4 ~ hormone + sex + hormone * sex,
    data = data_birds
) |> tidy()
kable(birds_anova)
```
Мы видим, что ни фактор инъекции, ни фактор пола не оказали значимого влияния на зависимую переменную.
Изменчивость их невелика.
Взаимодействие оказало значительное влияние.
Если мы построим график наших результатов, то увидим следующую картину:
```{r}
data_birds |>
    ggplot(aes(hormone, var4, fill = sex)) +
    geom_boxplot()
```
Сам факт инъекции по разному повлиял на концентрацию кальция в плазме в зависимости от пола.
В случае мужского пола это привело к увеличению интересующего нас показателя и к снижению концентрации в случае женского пола.
В этом и заключается идея взаимодействия факторов - когда некоторые переменные оказывают различное влияние на интересующий нас показатель в зависимости от уровней или градации другой независимой переменной.

### Требования к данным
Также важно отметить некоторые требования к данным.
Дисперсионный анализ требует нормальность распределения в каждой из групп и гомогенность дисперсий - то есть требование, чтобы дисперсия была примерно одинкаовой в каждой из групп.
Приятный факт, что ANOVA достаточно устойчива к обоим этим нарушениям.
Но если наблюдений меньше 30 в каждой из групп лучше проверять данные на нормальность распределения и на гомогенность дисперсии.
Для проверки гомогенности можно построить BoxPlot и убедиться в отсутствии большого числа выбросов или применить тест Левена и если дисперсии равны, то все хорошо.

### Интерпретация результатов
Само по себе применение дисперсионного анализа не дает оснований говорить о причинной зависимости данных.
Например, если мы решим выяснить кто лучше разбирается в статистике - математики, филологи или психологи, и для этого применим дисперсионный анализ, то значимые различия между группами (например математики будут разбираться лучше всего) может означать как тот факт, что те кто занимается математикой лучше научились статистике и теперь лучше её понимают, так и тот, что те, кто лучше понимает статистику становятся математиками, а не филологами или психологами.
Дисперсионный анализ проверяет гипотезу о взаимосвязи номинативной профессии и зависимой переменной (средней успеваемости по статистике).
Поэтому всегда важно задаваться вопросом “а можно ли объяснить данные с другой стороны”.

## АБ тесты и статистика
Мы подробно изучили теоретические аспекты статистических методов.
Пришло время узнать, как статистика применяется на практике для реальных исследований и экспериментов. 
АБ тестирование - это проведение экспериментов при помощи статистики, пожалуй, самый яркий пример того, зачем статистика нужна в реальной жизни!
A/B тесты - один из основных инструментов в продуктовой аналитике. Этот метод маркетингового исследования заключается в том, что контрольная группа элементов сравнивается с набором тестовых групп, где один или несколько показателей изменены для того, чтобы выяснить, какие из изменений улучшают целевой показатель.
Например, мы можем поменять цвет кнопки для регистрации с красного на синий и сравнить, насколько это будет эффективно. 
Данный раздел предлагается к просмотру на YouTube: https://www.youtube.com/watch?v=gljfGAkgX_o

# Корреляция и регрессия
## Понятие корреляции
Начнем знакомствос таким понятием, как корреляция.
При помощи корреляции мы научимся исследовать взаимосвязь двух количественных переменных, узнаем что такое положительная и отрицательная корреляции, разберемся как найти коэффициент корреляции и о чем он нам говорит.

Рассмотрим несколько примеров.

Предположим мы захотели узнать как взаимосвязан возраст и физическая активность.
Набрали выборку испытуемых и у каждого испытуемого узнали его возраст и число тренировок в неделю.
Если нанести все точки на график, то легко заметить, что все значения числа тренировок с ростом возраста будут понижаться.

Такая взаиомвязь называется отрицательной корреляцией.
Если с ростом одной переменной растет и вторая, то такая корреляция называется положительной.
По диаграмме рассеивания мы можем понять насколько сильно взаимосвязаны наши переменные, однако нужно что-то более конкретное, показатель.
Он называется коэффициентом корреляции.
Найдем формулу для расчета.
Для этого используем пример увеличения роста с увеличением веса.
Рост обозначим за y, вес за x.
График выглядит следующим образом:

```{r}
wh_data <- tibble(
    weight = runif(40, 50, 95),
    height = runif(40, -10, 10) + 150
) |>
    mutate(
        height = 0.4 * weight + height
    )
wh_data |> ggplot(aes(x = weight, y = height)) +
    geom_point() +
    labs(
        x = "Weight (x)",
        y = "Height (y)"
    )
```
Добавим на график две линии: красная вертикальная будет соответствовать среднем значению веса, а синяя горизонтальная - среднему значению роста.
```{r}
wh_data |> ggplot(aes(x = weight, y = height)) +
    geom_point() +
    labs(
        x = "Weight (x)",
        y = "Height (y)"
    ) +
    geom_hline(
        aes(
            yintercept = mean(height),
            color = "red"
        ),
        linetype = 2
    ) +
    geom_vline(
        aes(
            xintercept = mean(weight),
            color = "blue"
        ),
        linetype = 2
    )
```
Видим, что вся диаграмма разбилась линиями на 4 сектора.
Если большая часть наших наблюдений находится в верхнем правом и в нижнем левом секторах, значит наща корреляция положительная, поскольку если человек находится в верхнем правом секторе, то он и тяжелее и выше среднего.
Аналогичная ситуация со втором секторе.
Теперь для каждой точки рассчитаем следующий параметр:
$$
\left(x_i - \overline{X}\right)\cdot\left(y_i - \overline{Y}\right)
$$

Так как отклонение для точек в верхнем правом секторе положительное, то и показатель будет положительный.
Для точек в нижнем левом отклонения отрицательные, значит и показатель будет положительным.
Для точек из остальных секторов показатели будут отрицательными.
Большая часть показателей для точек будет положительной.
Теперь сложим все такие показатели и усредним (добавляем -1 в знаменатель, это связано с числом степеней свободы):
$$
\frac{\sum\left(x_i - \overline{X}\right)\cdot\left(y_i - \overline{Y}\right)}{N-1}
$$

```{r}
sum(
    (wh_data$weight - mean(wh_data$weight)) *
        (wh_data$height - mean(wh_data$height))
) / (length(wh_data$weight) - 1)
```
Такой показатель называется ковариацией.
```{r}
cov(wh_data$weight, wh_data$height)
```
Таким образом мы расчитали количественный показатель силы и направления взаимосвязи двух переменных.
Чем больше значение ковариации, тем сильнее взаимосвязь и если она положительная, то и корреляция положительная.

Теперь введем именно показатель корреляции:
\begin{equation}
r_{x y} = \frac{cov(x, y)}{\sigma_x \sigma_y}
\end{equation}

Таким образом все значения показателя корреляции всегда лежат в интервале [-1; 1].

```{r}
cov(wh_data$weight, wh_data$height) /
    (sd(wh_data$weight) * sd(wh_data$height))
```
или
```{r}
cor(wh_data$weight, wh_data$height)
```
Теперь укажем более традиционную формулу для корреляции (его так же называют коэффициентом корреляции Пирсона):

\begin{equation}
r_{x y} = \frac{\sum \left(x_i - \overline{X}\right)\cdot \left(y_i - \overline{Y}\right)}{\sqrt{\sum \left(x_i - \overline{X}\right)^2 \left(y_i - \overline{Y}\right)^2}}
\end{equation}

Квадрат коэффициента корреляции называется коэффициентом детерминации и показывает, в какой степени дисперсия одной переменной обусловлена влиянием другой переменной.
Принимает значения [0,1].
Теперь перейдем к вопросу, как проверять статистические гипотезы используя коэффициент корреляции Пирсона. возвращаемся к примеру с ростом и весом.
Сформулируем нулевую гипотезу: $r_{x y} = 0$.
Альтернативная гипотеза говорит: $r_{x y} \neq 0$.
В нашем эксперименте мы получили определенную связь между ростом и весом.
Теперь надо найти p-уровень значимости для гипотезы.
Мы будем расчитывать t-критерий при числе степеней свободы $N -2$ (на вопрос почему ответим позже в разделе про линейную регрессию).
Еще один пример:
Чему равен коэффициент корреляции в данной выборке
```{r, echo=FALSE}
kable(tibble(x = c(4, 5, 2, 3, 1), y = c(2, 1, 4, 3, 5)))
```
Решение:
```{r}
tibble(
    x = c(4, 5, 2, 3, 1),
    y = c(2, 1, 4, 3, 5)
) |>
    ggplot(aes(x, y)) +
    geom_point()
cor(c(4, 5, 2, 3, 1), c(2, 1, 4, 3, 5))
```

## Условия приенения коэффициента корреляции
Завершая разговор о корреляциях остановимся на нескольких важных моментах, посвященных правильной интерпретации получаемых данных в корреляционных исследованиях.

Первая из них - ошибка корреляции.
Её суть заключается в том, что положительная или отрицательная взаимосвязь еще не обязательно говорит о причинно-следственной зависимости.
Допустим мы решили выяснить, действительно ли компьютерные игры негативно влияют на подростков (агрессивное поведение). 
Взяли 500 школьников и посмотрели как часто они дерутся со сверстниками и как часто они играют в игры.
Если мы получили значимую положительную корреляцию, это еще не значит, что именно игры стали причиной агрессивного поведения.
Возможно это агрессивные школьники любят играть в компьютерные игры.

Сама по себе корреляция не означает наличие причинно-следственной зависимости, но может её означать.
Корреляция может подтверждать выдвинутую теорию.

Второй момент - это влияние так называемой третьей переменной.
Это такая неявная перменная, которая не входит в рассмотрение, но именно она является причиной корреляции.
Например: размер ноги школьника хорошо коррелирует со знаниями математики.
Потому что чем он старше, тем у него и размер ноги больше, и знаний больше.

## Регрессия с одной независимой перменной
В этом и следующих уроках мы научимся работать  с одномерным регрессионным анализом, который позволяет проверять гипотезы о взаимосвязи одной  количественной зависимой переменной и нескольких независимых.

Сначала мы познакомимся с самым простым вариантом -  простой линейной регрессией, при помощи которой можно исследовать взаимосвязь двух переменных.
Затем перейдем к множественной регрессии с несколькими независимыми переменными.

Регрессионный анализ это набор методов, позволяющих исследовать взаимосвязь различных переменных между собой.
Начнем мы с простой линейной регрессии, которая как и коэффициент корреляции позволяет нам исследовать взаимосвязь двух количественных
переменных.
Однако позволяет делать это более интересным образом.

### Линия регрессии
Одномерный регрессионный анализ применяется для исследования взаимосвязи двух переменных.
Пусть в нашем случае это буду $x$ и $y$.
Переменная по оси $y$ называется **зависимая переменная**, а $x$ - это **независимая переменная** или **предиктор**.

Регрессионный анализ обычно применяется, чтобы посмотреть как одна переменная объясняет другую переменную.
Например, хотим посмотреть как на качество товара влияет его цена.
Если две переменные как-то взаимосвязаны между собой удобно добавить линию на диаграмму.
Нам нужно, чтобы она следовала за облаком точек на диаграмме и чтобы она хорошо описывала распределение данных.

Мы знаем, что любая линия задается двумя параметрами:
$$
y = b_0 + b_1 x
$$

здесь, $b_0$ (intercept)  отвечает за точку пересечения линии с осью $Y$, а $b_1$(slope) за наклон линии.
Один из самых простых методов построения такой прямой это метод наименьших квадратов.

### Метод наименьших квадратов
\begin{definition}
МНК - метод нахождения оптимальных параметров линейной регресии, таких, что сумма квадратов ошибок (остатков) была минимальна.
\end{definition}

Остаток, это расстояние от отдельно выбранной точки до прямой.
Оно рассчитывается как:
$$
e_i = y_i - \hat{y_i}
$$

Сложением квадратов мы избегаем зануление одинаково отстоящих точек.
Расчет:
$$
b_1 = \frac{sd_y}{sd_x} r_{x y}, b_0 = \overline{y} - b_1 \overline{x}
$$

```{r}
data <- tibble(
    x = runif(50, 0, 60),
    y = runif(50, 0, 40)
) |>
    mutate(
        y = y + x
    )
lm(y ~ x, data) |>
    tidy() |>
    kable()
data |> ggplot(aes(x, y)) +
    geom_point() +
    geom_smooth(method = "lm", formula = y ~ x)
```

**Задача:** На графике изображена зависимость двух количественных переменных X  и Y .
Рассчитайте коэффициент b1 для регрессионной прямой, если коэффициент детерминации равен 0.25:
$$
M_x = 15 \text{(выборочное среднее)},
D_x = 25,
M_y = 10,
D_y = 36
$$

Формула для $b_1$: $b_1 = \frac{sd_y}{sd_x}r_{x y}$.
$sd = \sqrt{D}$, коэффициент детерминации: $r_{x y}^2$.
Итого:
```{r}
(sqrt(36) / sqrt(25)) * sqrt(0.25)
```

### Гипотеза о значимости взаимосвязи и коэффициент детерминации
Теперь, когда мы построили линию регрессии мы хотим ответить на вопрос <<на сколько статистически значима взаимосвязь двух наших переменных?>>.
За взаимосвязь отвечает именно $b_1$.
Если коэффициент корреляции 0, то прямая проходит параллельно оси $X$.
Таким образом, если переменные не связаны между собой, то $b_1$ становися равен 0.

То есть появляются две гипотезы:
$$
H_0: b_1 = 0
$$
$$
H_1: b_1 \neq 0
$$

Теперь появляется t-критерий, который говорит, что если бы мы многократно повторяли эксперимент и данные на самом деле не зависят, то выборочные значения коэффициентов $b_1$ распределились бы нормально относительно 0 и отклонялись бы то в правую, то в левую сторону.

Теперь можно расчитать t-критерий который будет сообщать насколько выборочное $b_1$ отклонится от ожидаемого.

И здесь он будет соответствовать формуле: $t = \frac{b_1}{se}$.

### Коэффициент детермнации
Одномерный дисперсионный анализ позволяет нам построить некоторую модель взаимосвязи двух количественных переменных, проверить гипотезу о наличии взаимосвязи, рассчитав соответствующий *p*-уровень значимости и получить значение коэффициента детерминации ($R^2$).
Мы уже немного говорили о том, что это такое, когда обсуждали коэффициент корреляции.
Мы выяснили, что $R^2$ это часть дисперсии одной переменной, обусловленная другой переменной.

В контексте регрессионного анализа, это:
\begin{definition}
    $R^2$ -- это доля дисперсии зависимой переменной ($Y$), объясняемая регрессионной моделью.
\end{definition}

Допустим, мы решили выяснить, как качество определяется ценой товара и построили регрессионную модель.
Для этого считаем:

\begin{equation}
 R^2 = 1 - \frac{SS_{res}}{SS_{total}}
\end{equation}

$SS_{res}$ -- сумма квадратов расстояний от наблюдений до регрессионной прямой.
$SS_{total}$ -- сумма квадратов расстояний от наблюдения до среднего значения.

Если значение $R^2$ велико, это говорит, что почти вся изменчивость зависимой переменной обуславливается её взаимосвязью с независимой переменной.
То есть регрессионная модель очень хорошо объясняет и описывает поведение и изменчивость зависимой переменной.
Чем больше $R^2$, тем лучше модель справляется с поставленной задачей -- объясняет и предсказывает значения зависимой переменной.

## Условия применения линейной регрессии с одним предиктором.
Подведем итоги.
Допустим у нас есть две количественные переменные и мы хотим узнать как они связаны между собой.
Более того, мы не просто хотим знать их связь, но и как одна переменная влияет на поведение другой.

Например мы хотим узнать, как уровень безработицы влияет на процент преступности.
Или как средний бал ЕГЭ у абитуриента определяет его успеваемость на первом курсе университета.

Таким образом мы хотим посмотреть как взаимосвязаны две переменные между собой.
Можем провести регрессионный анализ и посмотреть как линия регрессии максимально хорошо описывает эту взаимосвязь, построить уравнение регрессии, проверить гипотезу о статистически значимой взаимосвязи между ними и посчитать какой процент изменчивости зависимой переменной обуславливается нашей моделью.
Все это вместе делает этот метод незаменимым для решения колоссального числа задач.

Перед тем как посмотреть, как регресионный анализ позволяет нам исследовать реальные данные, поговорим про некоторые ограничения этого метода, какие требования к данным возникают когда мы хотим использовать его.

### Условия применения
Условия выглядят следующим образом:
* Линейная зависимость $X$ и $Y$
* Нормальное распределение остатков
* Гомоскедастичность -- постоянная изменчивость остатков на всех уровнях независимой переменной.

Чтобы познакомиться с этими требованиями наглядно, рассмотрим следующие примеры.
Код генерации примеров:
```{r}
# Based on code at
# https://github.com/ShinyEd/intro-stats/blob/master/slr_diag/app.R
make_example <- function(type) {
    make_data <- function(type) {
        n <- 250
        if (type == "linear.up") {
            x <- c(runif(n - 2, 0, 4), 2, 2.1)
            y <- 2 * x + rnorm(n, sd = 2)
        }
        if (type == "linear.down") {
            x <- c(runif(n - 2, 0, 4), 2, 2.1)
            y <- -2 * x + rnorm(n, sd = 2)
        }
        if (type == "curved.up") {
            x <- c(runif(n - 2, 0, 4), 2, 2.1)
            y <- 2 * x^4 + rnorm(n, sd = 16)
        }
        if (type == "curved.down") {
            x <- c(runif(n - 2, 0, 4), 2, 2.1)
            y <- -2 * x^3 + rnorm(n, sd = 9)
        }
        if (type == "fan.shaped") {
            x <- seq(0, 3.99, 4 / n)
            y <- c(
                rnorm(n / 8, 3, 1),
                rnorm(n / 8, 3.5, 2),
                rnorm(n / 8, 4, 2.5),
                rnorm(n / 8, 4.5, 3),
                rnorm(n / 4, 5, 4),
                rnorm((n / 4) + 2, 6, 5))
        }
        tibble(x = x, y = y)
    }

    my_data <- make_data(type)
    lm_results <- lm(y ~ x, data = my_data)
    xcon <- seq(min(my_data$x), max(my_data$x), 0.025)
    predictor <- tibble(x = xcon)
    pred_inter <- as.data.frame(
        predict(lm_results, newdata = predictor, interval = "prediction")
    ) |> mutate(x = xcon)

    r_squared <- round(summary(lm_results)$r.squared, 4)
    corr_coef <- round(sqrt(r_squared), 4)


    regression_plot <- my_data |>
        modelr::add_predictions(lm_results, var = "y_end")|>
        ggplot(aes(x = x, y = y)) +
            geom_point() +
            geom_smooth(method = lm, formula = y ~ x) +
            geom_segment(aes(xend = x, yend = y_end, color = "red")) +
            geom_ribbon(data = pred_inter,
                aes(x = x, y = fit, ymin = lwr, ymax = upr),
                alpha = 0.2
            ) +
            labs(
                title = paste0(
                    "Regression Model\n",
                    "(R = ", corr_coef, ", ",
                    "R^2 = ", r_squared, ")")
            ) + theme(legend.position = "none")
    residuals <- summary(lm_results)$residuals
    predicted <- predict(lm_results, newdata = data.frame(x = my_data$x))
    residual_plot <- tibble(x = predicted, y = residuals) |>
        ggplot(aes(x, y)) +
            geom_point() +
            geom_hline(yintercept = 0, linetype = 2) +
            labs(
                title = "Residuals vs. Fitted Values",
                x = "Fitted Values",
                y = "Residuals"
            )
    resid_hist <- tibble(res = residuals) |>
        ggplot(aes(x = res)) +
        geom_histogram(aes(y = after_stat(density)),
            bins = 30,
            fill = "orange", color = "black") +
        geom_density(kernel = "gaussian", color = "blue") +
        labs(
            title = "Histogram of Residuals",
            x = "Residuals"
        )
    qq_plot <- tibble(res = residuals) |>
        ggplot(aes(sample = res)) +
        geom_qq() + geom_qq_line() +
        labs(
            title = "Normal Q-Q Plot of Residuals",
            x = "Theoretical Quantiles",
            y = "Sample Quantiles"
        )

    grid.arrange(
        regression_plot,
        arrangeGrob(residual_plot, resid_hist, qq_plot,
            ncol = 3, nrow = 1),
        nrow = 2)
}
```

#### Пример 1
Пусть у нас линейная положительная взаимосвязь двух переменных.
```{r}
make_example("linear.up")
```
Видим, что наши данные распределены линейно и у нас видно положительные корреляции.
Для каждой точки мы расчитываем остатки (красные линии от точек).
По графику зависимости остатков (левый нижний график), видим, что остатки нормально распределились вокруг регрессионной линии.
То есть половина остатков положительная, половина отрицательная.
Если мы построим гистограмму распраделения остатков, увидим, что это нормальное распределение.
Дополнительно проверяем это через QQ-plot.
Таким образом у нас соблюдены требования нормальности распределения остатков и линейности взаимосвязи.

#### Пример 2
Теперь посмотрим, что произойдет, если взаимосвязь будет линейной, но отрицательной.
```{r}
make_example("linear.down")
```
Коэффициент корреляции стал отрицательным ($R$).
Видим, что остатки так же распределены нормально.
Таким образом, и в этом случае выполняются требования к линейной регрессии.

#### Пример 3
Следующий пример будет интереснее: искревленная зависимость.
```{r}
make_example("curved.up")
```
Зависимость явно нелинейная, но положительная.
Коэффициент регрессии здесь высокий, однако применять линейную регрессию здесь не совсем оправдано.
Если мы построим гистограмму и qq-plot остатков, то увидим, что их распределение не является нормальным.

#### Пример 4
```{r}
make_example("curved.down")
```
В этом примере мы так же наблюдаем ассиметричное распределение остатков.
На графике расброса точек (верхний) может показаться, что все обстоит не так плохо, но прогноз регрессии будет в таких случаях сильно расходиться с реальными данными.

#### Пример 5
```{r}
make_example("fan.shaped")
```
В этом примере зависимость в целом линейна.
Но так же видно, что чем дальше мы двигаемся по оси $x$, тем больше становятся остатки.
Мы видим, что остатки будут рассеиваться.
В этом случае линейная регрессия так же будет работать не совсем корректно.

Представленные в этих примерах графики являются хорошим примером анализа применимости линейной регрессии.
Если все условия выполняются, то линейная регрессия будет хорошо описывать зависимость.

Далее мы перейдем к решению реальных задач на линейную регрессию.

## Применение регрессионного анализа и интерпретация результатов
В качестве датафрейма мы будем использовать набор данных различных экономических показателей для каждого штата США (файл states.csv).
```{r}
states <- read_csv("states.csv", show_col_types = FALSE)
```
```{r, echo=FALSE}
kable(head(states))
states |>
    pivot_longer(cols = 2:6, names_to = "Statistic", values_to = "value") |>
    group_by(Statistic) |>
    summarise(
        N = dplyr::n(),
        Mean = mean(value),
        SD = sd(value),
        Min = min(value),
        Max = max(value)
    ) |>
    kable(caption = "Descriptive statistics")
```
Посмотрим на переменные, которые представлены в этом датафрейме.

* metro_res (metropolitan residence) говорит нам какой процент населения штата живет в столичной области.
* white - процент белокожего населения штата
* hs_grad - процент людей, имеющих среднее образование
* poverty - процент людей, проживающих в бедности
* female_house - процент семей, где женщина является домохозяйкой

Первая наша задача будет применить линейную регрессию с одной зависимой переменной и исследуем, как взаимосвязаны бедность и уровнь образования.

Для начала имеет смысл просто построить диаграмму рассеяния чтобы составить первое впечателение о характере взаимосвязи наших переменных.
```{r, echo = FALSE}
ggplot(states, aes(hs_grad, poverty)) +
    geom_point() +
    labs(
        title = "Correlation between High Scool graduates and poverty",
        x = "High School graduates",
        y = "Poverty"
    )
```
Очевидно, что за некоторыми исключениями, характер нашей зависимости ялвяется линейным.
При этом можно заметить, что взаимосвязь явно отрицательная.
```{r}
cor(states$hs_grad, states$poverty)
```
Линейность взаимосвязи позволяет нам применить регрессионный анализ.
Анализ остатков проведем чуть позже.
Сформулируем задачи и гипотезы, которые мы будем проверять при помощи регрессионного анализа.

В качестве зависимой переменной будет выступать показатель бедности, в качестве независимой -- уровень образования.

Первое, что мы сделаем, это построим регрессионную модель, которая максимально удачным образом  объясняет взаимосвязь двух наших пермененных.
В нашем случае это будет уравнение регрессионной прямой.

Далее мы будем выяснять насколько хорошо эта модель будет описывать поведение наших данных. Для этого мы будем считать коэффициент детерминации.

Далее мы ответим на вопрос, отличается ли коэффициент при независимой переменной в линейной модели от 0.
Это и будет нашей нулевой гипотезой так как если бы никакой взаимосвязи не было, то наша линия регрессии была бы параллельно оси $X$.
Если этот коэффициент не равен 0, значит возникает какой-то наклон, который говорит нам о взаимосвязи.

Далее мы основываясь на независимой переменной можно предсказать чему будет равна зависимая.

### Анализ
Зная коэффициент корреляции и описательную статистику, мы можем рассчитать коэффициенты регрессии, коэффициент детерминации и проверить наши статистические гипотезы при помощи T-теста.
```{r}
ggplot(states, aes(hs_grad, poverty)) +
    geom_point() +
    geom_smooth(method = "lm", formula = y ~ x) +
    labs(
        title = "Correlation between High Scool graduates and poverty",
        x = "High School graduates (%)",
        y = "Poverty (%)"
    )
states_lm <- lm(poverty ~ hs_grad, states)
states_lm |> tidy() |> kable()
print(summary(states_lm))
```
Мы обнаружили статистически значимую взаимосвязь наших переменных ($p < 0.05$).
Напротив Intercept так же есть значение $p$-value.
Тут проверяется гипотеза, что Intercept отличен от 0.
Это не позволяет сделать такого сильного вывода, как во втором случае, но так же дает интересную информацию.
Если предположить существование такого штата, где процент людей с высшим образованием равен 0, то мы ожидаем, что в таком штате будет примерно 64.78% людей проживающих за чертой бедности.

Удостоверившись в том, что взаимосвязь между нашими переменными статистически значимая, интерпретируем значение коэфициента наклона.
Значение $-0.62$ говорит о том, что взаимосвязь отрицательная и означает, что с каждым процентом, увеличения людей, имеющих высшее образование, мы ожидаем, что количество людей, проживающих в бедности будет уменьшаться на 0.62%.
Поэтому в случае применения регрессионного анализа в таких задачах, коэффициенты перед независимыми переменными имеют определенный смысл: как будет изменяться значение зависимой переменной от независимой.

Далее, значение коэффициента детерминации 0.56 означает, что практически 56% изменчивости нашей зависимой переменной объясняется нашей моделью.
В целом это не так уж и много -- почти половина изменчивости обуславливается не включенными в модель факторами.
У коэффициента детерминации так же есть свой *p*-уровень значимости и F-значение, полученное при применении дисперсионного анализа при проверки гипотезы о том, что модель позволяет объяснить поведение нашей зависимой переменной.
Как интерпретировать уровнь значимости для всей модели станет понятнее, когда мы разберемся с множественно регрессией, когда одновременно используется несколько предикторов.

При помощи регрессионного анализа, мы узнали как взаимосвязаны две переменные.
Мы посмотрели насколько значима эта взаимосвязь, она оказалось отрицательной, построили линейную модель, выяснили какой процент дисперсии обуславливается взаимосвязью с другой переменной и узнали какие ожидать изменение уровня бедности с единичным изменением уровня образования.

Теперь посмотрим на остатки, получившейся модели.
```{r}
states_resid <- states |>
    add_residuals(states_lm)
grid.arrange(
    ggplot(states_resid, aes(hs_grad, resid)) +
        geom_point() +
        geom_hline(yintercept = 0, linetype = 2, color = "red"),
    ggplot(states_resid, aes(sample = resid)) +
        geom_qq() +
        geom_qq_line(color = "red"),
    nrow = 1, ncol = 2
)
```
На графиках изображено распределение остатков:
Левый график: распределение остатков на различных уровнях независимой переменной
Правый график: qq - plot для остатков. 
Основываясь на этих данных, можно заключить, что в целом требования гомоскедастичности и нормальности распределения остатков выполнено (хотя и присутствуют заметные положительные отклонения от нуля).

Таким образом, делаем вывод, что применение регрессионного анализа в данном случае было обоснованным и мы можем доверять нашей модели.
Следовательно мы можем попробовать предсказать дальнейшие значения.

## Задача предсказания значений зависимой переменной
Регрессионную прямую еще иногда называют линией тренда.
По ней мы можем предсказать, чему будут равны значения зависимой переменной на определенном уровне значения независимой переменной.

Например, если бы мы решили выяснить, чему равняется процент людей, проживающих за чертой бедности в штате, где среднее образование имеют 95% населения.
В нашей выборке (она здесь совпадает с генеральной совокупностью) такого наблюдения не было.
Но основываясь на регрессионном анализе, мы можем сделать предсказание относительно такого события.
```{r}
tibble(hs_grad = c(95)) |>
    add_predictions(states_lm) |>
    kable()
```
Таким образом, основываясь на регрессионном анализе мы можем предсказывать несуществующие значения.
Этот метод используется повсеместно.
Однако эти предсказания имеют некоторые ограничения.
Если мы вернемся к примеру с бедностью, нет ничего удивительного в том, что при некоторых значениях уровня образования, предсказанное значение будет принимать отрицательные значения, что совершенно бессмысленно.

Другая частая ситуация -- предсказание работает только на каком-то промежутке значений.
Например, если мы будем изучать зависимость роста от возраста, то до 20 лет мы будем получать линейную взаимосвязь, но потом значение роста перестанет меняться.

Однако, в случае с уровнем бедности и образованности, мы можем объяснить установленную взаимосвязь двумя способами:

1. Чем лучше люди учатся, тем лучше они работают и тем больше их благосостояние.
2. Чем меньше благосостояние населения, тем меньше у них времени на учебу.

Сам факт взаимосвязи совершенно ничего не говорит о порядке причинно-следственных связей.

## Регрессионный анализ с несколькими независимыми переменными
### Множественная регрессия
Множественная регрессия позволяет исследовать влияние сразу нескольких независимых переменных на одну зависимую переменную.

Мы выяснили, как влияет уровень образования на уровень бедности.
В большинстве случаев используется множетсво различных факторов.
Мы можем так же узнать, как процент белого населения влияет на показатель зависимой переменной, то мы можем включить эту переменную в нашу модель.
Тогда уравнение для нашей модели примет несколько другой вид:
$$
\hat{y} = b_0 + b_1 x_1 + b_2 x_2
$$

Если мы хотим включать произвольное число переменных, то модель принимает следующий вид:

\begin{equation}
    \hat{y} = b_0 + \sum_{i = 1}^{N} b_i x_ i
\end{equation}

В случае нашей задачи, можно визуализировать зависимость следующим образом:
```{r, echo = FALSE}
s3d <- scatterplot3d(states$hs_grad, states$white, states$poverty,
    pch = 16, highlight.3d = TRUE, type = "h")
fit <- lm(poverty ~ hs_grad + white, states)
s3d$plane3d(fit)
```
Кроме того, теперь регрессия является не прямой линией, а плоскостью.
Как и раньше, с увеличением числа людей с средним образованием, уровень бедности падает.
Но при этом с ростом числа белого населения, уровень бедности так же начинает немного снижаться.

Таким образом видим, что и уровень образования и уровень белого населения отрицательно взаимосвязан с зависимой переменной.
Поэтому в задачах с множественной регрессией мы так же будет оценивать значения коэффициентов.

### Требования к данным

* Линейная зависимость переменных
* Нормальное распределение остатков
* Гетероскедастичность
* Проверка на мультиколлинеарность
* Нормальное распределение переменных (желательно)

Построим регрессионную модель, которая включает сразу все параметры из датасета.

```{r}
summary(lm(
    poverty ~ metro_res + white + hs_grad + female_house,
    data = states))
```

Теперь у нас есть значения Intercept и коэффициентов перед каждой из переменной.
Физический смысл Intercept в целом сохраняется.
При помощи T критерия мы оцениваем отличие параметра от 0.
Если рассматривать значение каждого коэффициента по отдельности, то можно сказать следующее.

* **metro_res** -- статистически значимо взаимосвязан с зависимой переменной. Он отрицательный, значит чем больше людей живет в столичной области, тем меньше уровнь бедности. Если бы мы допустили, что все остальные перменные зафиксированы, то мы бы увидели, что с каждым увеличением процента людей, проживающих в столичной области, уровень бедности снижался бы на 0.06%.
* **white** -- этот параметр судя по $p$-value значимо не отличается от 0, поэтому мы ничего не можем сказать о его значимости.
* **hs_grad** -- Видим, что он самый большой по модулю и что у него самый низкий p.value, что говорит нам о том, что этот коэффициент вносит существенный вклад в нашу модель.
* **female_house** -- аналогично white.

Кроме того видим, что у нас есть параметр adjusted r-squared.
Это скорректированный коэффициент детерминации.
Рассчитывается он при включении в модель дополнительных независимых переменных.

## Выбор наилучшей модели
Подведем итог.
Мы решили для начала выяснить как на нашу зависимую переменную (процент бедности) влияет набор социально-экономических показателей и сначала построили простую модель с двумя переменными.
Получилось не так плохо: мы смогли найти значимую взаимосвязь между этими переменными и построить регрессионную модель.
Посчитали какой процент изменчивости обуславливается этой моделью.

Затем мы включили все показатели в модель.
Это привело к некоторому улучшению.
Мы смогли поймать еще одну переменную, которая так же имеет статистически значимую взаимосвязь с нашей переменной (процент людей, проживающих в столичном регионе).

Однако полученная нами модель далеко не самая лучшая из доступных нам.
Здесь надо вспомнить про требование мультиколлинеарности.
Мультиколлинеарность означает очень сильную взаимосвязь между какими-то из независимых переменных.

Включение таких переменных в модель нецелесообразно: если две независимые переменные между собой сильно взаимосвязаны, то достаточно только одной из них, чтобы хорошо объяснить зависимость.
С точки зрения математики, если у нас сильная корреляция между предикторами, то становится проблематично рассчитать все показатели.

То есть, при построении регрессонной модели, далеко не факт, что включение всех переменных в регрессионный анализ приведет к получению наилучшего результата (большего $R^2$).

Посмотрим как можно подобрать оптимальную модель.

```{r}
ggpairs(states |> select(-c(state)))
```
Видим знакомую уже нам сильную корреляцию между poverty и hs_grad.
Кроме того видим, что переменная female_house хорошо коррелирует со всеми переменными.
Не исключено, что наличие этой переменной в нашей модели ухудшает её работу.

Один из возможных методов улучшения модели выглядит следующим образом.
Мы строим модель в которую включаем все переменные и считаем Adj. R-squared.
Далее мы будем по очереди удалять каждую переменную.
```{r}
cols <- colnames(states) |>
    purrr::discard(\(x) x %in% c("state", "poverty"))
cols_to_del <- cols |> append("", after = 0)
model_selection <- tibble(to_del = cols_to_del) |>
    mutate(
        model = str_c(
            "poverty ~ ",
            map(to_del,
                \(y) str_flatten(
                    purrr::discard(
                        cols,
                        \(x) x == y),
                    collapse = " + ")
                )
            ),
        adj_r_sq = map(
            model,
            \(x) round(summary(
                lm(formula(x), data = states)
            )$adj.r.squared, digits = 2))
    )
kable(model_selection)
```
Видим, что оптимальная модель возникает при удалении female_house.
Продолжаем удалять переменные предварительно выкинув переменную female_house.
```{r}
cols <- colnames(states) |>
    purrr::discard(\(x) x %in% c("state", "poverty", "female_house"))
cols_to_del <- cols |> append("", after = 0)
model_selection <- tibble(to_del = cols_to_del) |>
    mutate(
        model = str_c(
            "poverty ~ ",
            map(to_del,
                \(y) str_flatten(
                    purrr::discard(
                        cols,
                        \(x) x == y),
                    collapse = " + ")
                )
            ),
        adj_r_sq = map(
            model,
            \(x) round(summary(
                lm(formula(x), data = states)
            )$adj.r.squared, digits = 2))
    )
kable(model_selection)
```
Из этой таблицы мы видим, что лучший результат дает модель, где отсутствует только female_house.

Посмотрим на результаты этой модели.
```{r}
summary(lm(poverty ~ metro_res + white + hs_grad,
    data = states))
```
Обратим внимание, что исключив переменную female_house, переменная white так же стала статистически взаимосвязана.

Завершая анализ этой модели, убедимся, что с остатками так же все в порядке.
```{r}
states_final_lm <- lm(
    poverty ~ metro_res + white + hs_grad,
    data = states)
states_fin_resid <- states |>
    add_residuals(states_final_lm) |>
    add_predictions(states_final_lm)
grid.arrange(
    ggplot(states_fin_resid, aes(pred, resid)) +
        geom_point() +
        geom_hline(yintercept = 0, linetype = 2, color = "red"),
    ggplot(states_fin_resid, aes(sample = resid)) +
        geom_qq() +
        geom_qq_line(color = "red"),
    nrow = 1, ncol = 2
)
```

## Классификация: логистическая регрессия и кластерный анализ
